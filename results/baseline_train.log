2026-01-29 11:53:56.591572: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2026-01-29 11:53:56.650218: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Loading model: Qwen/Qwen2.5-0.5B-Instruct
Loading model on GPU...
trainable params: 1,081,344 || all params: 495,114,112 || trainable%: 0.2184
Preparing datasets...
Train examples: 227
Val examples: 43
Starting training...
  0%|          | 0/42 [00:00<?, ?it/s]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
/home/biff/.pyenv/versions/3.9.12/lib/python3.9/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.
  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]
  2%|▏         | 1/42 [00:52<35:38, 52.15s/it]  5%|▍         | 2/42 [01:07<20:20, 30.50s/it]  7%|▋         | 3/42 [01:22<15:13, 23.41s/it] 10%|▉         | 4/42 [01:32<11:30, 18.18s/it] 12%|█▏        | 5/42 [01:45<09:56, 16.13s/it] 14%|█▍        | 6/42 [01:48<07:05, 11.83s/it] 17%|█▋        | 7/42 [01:52<05:17,  9.08s/it] 19%|█▉        | 8/42 [01:55<04:07,  7.29s/it] 21%|██▏       | 9/42 [01:58<03:20,  6.07s/it] 24%|██▍       | 10/42 [02:02<02:48,  5.25s/it]                                                24%|██▍       | 10/42 [02:02<02:48,  5.25s/it] 26%|██▌       | 11/42 [02:05<02:27,  4.76s/it] 29%|██▊       | 12/42 [02:09<02:12,  4.42s/it] 31%|███       | 13/42 [02:13<02:01,  4.18s/it] 33%|███▎      | 14/42 [02:16<01:53,  4.05s/it] 36%|███▌      | 15/42 [02:17<01:22,  3.04s/it] 38%|███▊      | 16/42 [02:21<01:24,  3.27s/it] 40%|████      | 17/42 [02:25<01:25,  3.41s/it] 43%|████▎     | 18/42 [02:29<01:25,  3.55s/it] 45%|████▌     | 19/42 [02:32<01:22,  3.60s/it] 48%|████▊     | 20/42 [02:36<01:19,  3.61s/it]                                                48%|████▊     | 20/42 [02:36<01:19,  3.61s/it] 50%|█████     | 21/42 [02:40<01:15,  3.62s/it] 52%|█████▏    | 22/42 [02:43<01:12,  3.63s/it] 55%|█████▍    | 23/42 [02:47<01:10,  3.68s/it] 57%|█████▋    | 24/42 [02:51<01:05,  3.67s/it] 60%|█████▉    | 25/42 [02:54<01:01,  3.65s/it] 62%|██████▏   | 26/42 [02:58<00:58,  3.64s/it] 64%|██████▍   | 27/42 [03:02<00:54,  3.65s/it] 67%|██████▋   | 28/42 [03:05<00:51,  3.67s/it] 69%|██████▉   | 29/42 [03:09<00:47,  3.66s/it] 71%|███████▏  | 30/42 [03:10<00:33,  2.79s/it]                                                71%|███████▏  | 30/42 [03:10<00:33,  2.79s/it] 74%|███████▍  | 31/42 [03:13<00:34,  3.10s/it] 76%|███████▌  | 32/42 [03:18<00:34,  3.43s/it] 79%|███████▊  | 33/42 [03:21<00:31,  3.52s/it] 81%|████████  | 34/42 [03:25<00:28,  3.55s/it] 83%|████████▎ | 35/42 [03:29<00:25,  3.58s/it] 86%|████████▌ | 36/42 [03:32<00:21,  3.59s/it] 88%|████████▊ | 37/42 [03:36<00:18,  3.61s/it] 90%|█████████ | 38/42 [03:40<00:14,  3.64s/it] 93%|█████████▎| 39/42 [03:43<00:10,  3.63s/it] 95%|█████████▌| 40/42 [03:47<00:07,  3.61s/it]                                                95%|█████████▌| 40/42 [03:47<00:07,  3.61s/it] 98%|█████████▊| 41/42 [03:51<00:03,  3.65s/it]100%|██████████| 42/42 [03:54<00:00,  3.64s/it]                                               100%|██████████| 42/42 [03:55<00:00,  3.64s/it]100%|██████████| 42/42 [03:55<00:00,  5.60s/it]
{'loss': 0.1255, 'grad_norm': 4.279308319091797, 'learning_rate': 0.000172972972972973, 'epoch': 0.7}
{'loss': 0.1149, 'grad_norm': 2.159496545791626, 'learning_rate': 0.00012432432432432433, 'epoch': 1.35}
{'loss': 0.089, 'grad_norm': 2.591503620147705, 'learning_rate': 7.027027027027028e-05, 'epoch': 2.0}
{'loss': 0.0628, 'grad_norm': 0.40922433137893677, 'learning_rate': 1.6216216216216218e-05, 'epoch': 2.7}
{'train_runtime': 235.0639, 'train_samples_per_second': 2.897, 'train_steps_per_second': 0.179, 'train_loss': 0.09567605721808616, 'epoch': 2.85}
Saving model to models/baseline_track1
Training complete!
